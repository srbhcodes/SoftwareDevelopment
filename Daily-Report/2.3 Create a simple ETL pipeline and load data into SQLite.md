
### **ETL Pipeline Creation with SQLite - Detailed Report**

#### **Objective:**
The goal is to create an ETL (Extract, Transform, Load) pipeline that extracts data from a CSV file, transforms it, and loads it into an SQLite database. This process involves:
1. **Extracting** data from a CSV file.
2. **Transforming** the data (optional data cleaning or modifications).
3. **Loading** the data into an SQLite database.

---

### **Step 1: Set Up the Project Environment**

**What we did:**
- Created a **virtual environment** to isolate the project dependencies and ensure we are using the correct versions of libraries.
- Installed the required libraries: `pandas`, `sqlite3`, and `requests`.

**Why we did it:**
- Using a virtual environment helps in managing dependencies for each project separately, preventing conflicts between different project requirements.
- `pandas` is used for handling data efficiently, and `sqlite3` allows interaction with SQLite databases.

**Commands used:**
```bash
mkdir etl_pipeline_project   # Create a directory for the project
cd etl_pipeline_project      # Navigate to the project directory
python3 -m venv venv         # Create a virtual environment
source venv/bin/activate     # Activate the virtual environment
pip install pandas sqlite3  # Install necessary libraries
```

---

### **Step 2: Create the ETL Pipeline Script**

**What we did:**
- Created a Python script named `etl_pipeline.py`.
- The script is responsible for three key operations:
  1. **Extract**: Read the data from a CSV file using `pandas`.
  2. **Transform**: Perform a basic transformation to clean the data by removing rows with missing values.
  3. **Load**: Insert the transformed data into an SQLite database using `sqlite3`.

**Why we did it:**
- The ETL pipeline is central to automating data processing tasks. The script reads, processes, and stores data in one smooth workflow.

**ETL Pipeline Script Overview:**
```python
import pandas as pd
import sqlite3

# Extract data from CSV
def extract_csv(csv_file):
    df = pd.read_csv(csv_file)  # Read CSV into a DataFrame
    return df

# Transform data (remove missing values)
def transform_data(df):
    df = df.dropna()  # Drop rows with missing values
    return df

# Load data into SQLite database
def load_to_sqlite(df, db_name, table_name):
    conn = sqlite3.connect(db_name)  # Connect to SQLite database
    df.to_sql(table_name, conn, if_exists='replace', index=False)  # Insert data into database
    conn.close()

def main():
    csv_file = 'data.csv'  # Path to your CSV file
    db_name = 'etl_data.db'  # SQLite database name
    table_name = 'etl_table'  # Table name in the database

    print("Extracting data...")
    df = extract_csv(csv_file)
    print("Transforming data...")
    df = transform_data(df)
    print("Loading data into SQLite...")
    load_to_sqlite(df, db_name, table_name)
    print("ETL process completed!")
```

---

### **Step 3: Create a Sample CSV File for Testing**

**What we did:**
- Created a sample CSV file (`data.csv`) to test the ETL pipeline. The CSV file contained a few rows of data with columns like `name`, `age`, and `city`.

**Why we did it:**
- A CSV file is a common format for storing data. Using this sample file, we can test whether the ETL pipeline works as expected.

**Sample CSV Content:**
```csv
name,age,city
Alice,30,New York
Bob,25,Los Angeles
Charlie,35,Chicago
```

---

### **Step 4: Run the ETL Pipeline Script**

**What we did:**
- Ran the `etl_pipeline.py` script to test the entire ETL process. The script:
  - Extracted data from `data.csv`,
  - Transformed it by dropping rows with missing values,
  - Loaded the data into an SQLite database named `etl_data.db` and created a table `etl_table`.

**Why we did it:**
- Running the script is the final step in the ETL process, which automates reading, transforming, and saving data into the database.

**Command used to run the script:**
```bash
python etl_pipeline.py
```

---

### **Step 5: Verify the Data in SQLite**

**What we did:**
- Opened the SQLite database (`etl_data.db`) using the SQLite shell to verify that the data was loaded correctly into the database.

**Why we did it:**
- Verifying the loaded data ensures that the ETL pipeline works as expected and the data is correctly inserted into the database.

**Commands to check data:**
```bash
sqlite3 etl_data.db        # Open the SQLite database
SELECT * FROM etl_table;   # Query the table to view the data
```

---

### **Step 6: Conclusion and Final Notes**

**What we accomplished:**
- Successfully created an ETL pipeline that extracts data from a CSV file, transforms it, and loads it into an SQLite database.

**Why this is important:**
- ETL pipelines are critical for automating data processing tasks. This simple example can be extended to work with more complex transformations or larger datasets.
- The skills learned here are fundamental to data engineering and data science roles, where handling data efficiently is key.

---

### **Further Improvements:**
- **Error Handling**: Adding error handling in the ETL process (e.g., handling missing files, database connection issues) would make the script more robust.
- **Advanced Transformations**: You can add more complex data transformations like data aggregation, data normalization, or data enrichment.
- **Logging**: Implementing logging would help track the execution of the ETL pipeline, especially for larger datasets.

---

This concludes the detailed step-by-step process we followed to create a simple ETL pipeline with SQLite. Let me know if you need more details or further improvements!