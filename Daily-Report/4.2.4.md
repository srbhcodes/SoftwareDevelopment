### **Building a Basic ML Pipeline Using MLflow**

In this task, we aimed to set up a **machine learning pipeline** using **MLflow**, a powerful open-source tool that helps track experiments, models, and workflows for machine learning projects. The idea was to use MLflow to streamline our workflow and make it easy to manage different experiments, models, and configurations.

Here’s a detailed look at how we built this pipeline and the challenges we encountered during the process.

---

### **Step 1: Setting Up MLflow**

To get started with MLflow, we needed to first install the necessary dependencies. MLflow is a Python library, so installing it is as simple as running:

```bash
pip install mlflow
```

Once installed, we were able to utilize MLflow for experiment tracking and model management. In addition, we had to make sure that we had the following environment setup:

- **MLflow Tracking Server**: To track and log our experiments, we would ideally want to have an MLflow tracking server set up, but for simplicity, we used MLflow's default local storage during development.

---

### **Step 2: Define a Simple ML Model for the Pipeline**

For our pipeline, we decided to start with the **MNIST dataset** for simplicity and ease of implementation.

Here’s the general structure we followed:

1. **Load Data**: We used TensorFlow/Keras to load the MNIST dataset. This is a simple image classification problem where the goal is to classify handwritten digits into one of 10 categories (0-9).
    
    ```python
    from tensorflow.keras.datasets import mnist
    from tensorflow.keras.utils import to_categorical
    
    # Load MNIST dataset
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
    
    # Preprocess the data
    train_images = train_images / 255.0  # Normalize the images
    test_images = test_images / 255.0
    
    train_labels = to_categorical(train_labels, 10)  # One-hot encode labels
    test_labels = to_categorical(test_labels, 10)
    ```
    
2. **Build the Model**: We built a simple **neural network** model using TensorFlow/Keras.
    
    ```python
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Flatten
    
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    ```
    
3. **Train the Model**: We trained the model on the MNIST dataset and used `mlflow` to log the experiment.
    

---

### **Step 3: Integrate MLflow to Track the Experiment**

We integrated MLflow into our pipeline to track various aspects of the model training, including the model parameters, metrics, and other important information.

Here’s how we did it:

1. **Start MLflow Experiment**: We began an experiment and set it up using `mlflow.start_run()`. This is the key function that allows MLflow to track the experiment from start to finish.
    
    ```python
    import mlflow
    import mlflow.tensorflow
    
    # Start a new MLflow run
    with mlflow.start_run():
        # Train the model
        model.fit(train_images, train_labels, epochs=5, batch_size=32, validation_data=(test_images, test_labels))
        
        # Log the model parameters, metrics, and artifacts
        mlflow.log_param("epochs", 5)
        mlflow.log_param("batch_size", 32)
        mlflow.log_metric("accuracy", model.evaluate(test_images, test_labels)[1])
        
        # Log the model as an artifact
        mlflow.tensorflow.log_model(model, "model")
    ```
    
    - **Parameters**: We logged hyperparameters like the number of epochs and batch size.
    - **Metrics**: We logged the accuracy of the model on the test data after training.
    - **Model Artifacts**: We logged the trained model itself, which can later be loaded and used for inference or further experiments.

---

### **Step 4: Running and Managing the MLflow UI**

Once the experiment was logged, we could view it through the **MLflow UI**, which provides a nice web-based interface to track and manage experiments. To view the UI:

```bash
mlflow ui
```

By default, MLflow runs the UI at `http://localhost:5000`. This interface showed us all the logs, parameters, metrics, and artifacts for our experiment. It made it easy to compare different runs and monitor model performance visually.

---

### **Step 5: Load the Saved Model**

Once the model was saved in MLflow, we could easily load it back into a different session for further use or evaluation.

```python
# Load the saved model
loaded_model = mlflow.tensorflow.load_model("runs:/<run_id>/model")

# Evaluate the loaded model
test_loss, test_acc = loaded_model.evaluate(test_images, test_labels)
print(f"Test accuracy of loaded model: {test_acc}")
```

The model could be loaded back with the specific run ID, ensuring that we were working with the exact same model that was previously trained and logged.

---

### **Challenges We Faced**

While the process was mostly smooth, we faced some challenges along the way:

1. **MLflow Server Setup**: Initially, we tried to set up an MLflow tracking server for more robust tracking, but we quickly realized that it wasn’t necessary for our simple use case. We switched to using the local backend, which allowed us to experiment more quickly.
    
2. **Logging Metrics and Parameters**: One challenge we faced was ensuring that we logged the correct **metrics** and **parameters** at the right stages. For example, logging the model's accuracy during training wasn’t straightforward because we had to evaluate the model after training to get the accuracy. We resolved this by using `mlflow.log_metric()` after evaluation.
    
3. **Model Serialization**: The biggest challenge was ensuring that **model serialization** (saving and loading the model) worked seamlessly. We initially had issues with TensorFlow models and their compatibility with MLflow’s logging and loading functions. The solution was to use `mlflow.tensorflow.log_model()` to log the model and `mlflow.tensorflow.load_model()` to reload it correctly. Once we figured that out, it worked smoothly.
    
4. **TensorFlow Version Compatibility**: As always, there were some challenges with TensorFlow version mismatches. We had to ensure that the version of **TensorFlow** used to train the model was compatible with the version in our MLflow environment. This involved testing different versions until we found the right one that worked without breaking any functionalities.
    
5. **Model Tracking Over Multiple Runs**: As we kept running different experiments (e.g., varying the number of epochs or batch sizes), we had to be careful about managing different **runs** and ensuring we were correctly associating the results with the right parameters. MLflow handled this well, but it took a little time to get used to the structure of logging runs.
    

---

### **Conclusion**

Building a basic ML pipeline using **MLflow** helped us better manage machine learning workflows by tracking experiments, metrics, and models. The main tasks we accomplished were:

- Loading and preparing data (MNIST dataset).
- Building and training a TensorFlow model.
- Logging parameters, metrics, and models using MLflow.
- Using the MLflow UI to track experiments and compare results.
- Saving and loading the model for future use.

While we faced challenges with **logging metrics** and **model serialization**, we overcame these by learning how to properly integrate **TensorFlow** with MLflow and ensuring the correct versions were used. By the end of the task, we had a fully functional ML pipeline that could be expanded and used for future machine learning experiments.

This process showcased how MLflow can streamline the management of machine learning experiments, helping to track, save, and deploy models efficiently.